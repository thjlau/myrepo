{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Searches on china pubmed depending on what user inputs\n",
    "search_keyword = input (\"Please enter search: \")\n",
    "#Creates lower case keyword used for creating a keyword series in Function F and for naming the output csv file\n",
    "final_keyword = search_keyword.lower()\n",
    "#If the search_keyword has a space character in it(e.g contains more than one word), it is replaced with a '+'\n",
    "if ' ' in search_keyword:\n",
    "    search_keyword = search_keyword.replace(' ', '+')\n",
    "#Taking user input, 搜索. Then 选择“发表日期” and create search url\n",
    "search_url = 'http://www.chinapubmed.net/search/?q=' + search_keyword + '&sort=year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECORDS CURRENT PAGE NUMBER AS A GLOBAL VARIABLE\n",
    "current_page = 1\n",
    "#Creates soup object for current search_url\n",
    "result = requests.get(search_url)\n",
    "soup = BeautifulSoup(result.content, 'lxml') #use lxml, much faster than html.parser\n",
    "#FUNCTION A\n",
    "#THIS FUNCTION RETURNS IMPACT FACTORS ON GIVEN WEBPAGE AS A SERIES\n",
    "def impact_factor():\n",
    "    impact_factors = soup.find_all('span')\n",
    "    #append each impact factor into a list which is then turned into a series\n",
    "    factors_list = []\n",
    "    for factors in impact_factors:\n",
    "        if len(factors.text)>2 and (factors.text[0]).isdigit() == True and (factors.text[-1]).isdigit() == True and ('.' in factors.text) == True:\n",
    "            factors_list.append(float(factors.text))\n",
    "    return pd.Series(factors_list)\n",
    "\n",
    "#FUNCTION B\n",
    "#THIS FUNCTION RETURNS RELEVANT 原始url ON GIVEN WEBPAGE AS A SERIES\n",
    "def article_links():\n",
    "    test = soup.find_all('a')\n",
    "    final_urls = []\n",
    "    for link in test:\n",
    "        if any(char.isdigit() for char in link.attrs['href']) == True and ('search' not in link.attrs['href']) and ('javascript' not in link.attrs['href']):\n",
    "            final_urls.append('http://www.chinapubmed.net' + link.attrs['href'])\n",
    "    final_urls = list(dict.fromkeys(final_urls))\n",
    "    return pd.Series(final_urls)\n",
    "\n",
    "#FUNCTION C\n",
    "#THIS FUNCTION RETURNS ALL 英文标题 ON GIVEN WEBPAGE AS A SERIES (ONLY WORKS FOR EXTRACTING ENGLISH TITLE)\n",
    "def article_titles():\n",
    "    titles = soup.find_all('div', class_ = 'paper-list-title')\n",
    "    final_list = []\n",
    "    for t in titles:\n",
    "        final_list.append(t.text)\n",
    "    title_list = []\n",
    "    for name in final_list:\n",
    "        a,b,c,d= name.split('\\n')\n",
    "        title_list.append(c)\n",
    "    return pd.Series(title_list)  \n",
    "\n",
    "\n",
    "#FUNCTION D\n",
    "#THIS FUNCTION RETURNS ALL PMID VALUES ON GIVEN WEBPAGE AS A SERIES\n",
    "def get_pmid():\n",
    "    test = soup.find_all('a')\n",
    "    final_urls = []\n",
    "    for link in test:\n",
    "        if any(char.isdigit() for char in link.attrs['href']) == True and ('search' not in link.attrs['href']) and ('javascript' not in link.attrs['href']):\n",
    "            string = link.attrs['href']\n",
    "            final_urls.append(string[1:len(string)])\n",
    "    final_urls = list(dict.fromkeys(final_urls))\n",
    "    return pd.Series(final_urls)\n",
    "\n",
    "#FUNCTION E\n",
    "#THIS FUNCTION returns all 期刊 values on webpage as a series\n",
    "def get_periodical():\n",
    "    test = soup.find_all('span', class_ = 'journal_fifi')\n",
    "    urls = []\n",
    "    for search in test:\n",
    "        urls.append(search.string)\n",
    "    return pd.Series(urls)\n",
    "\n",
    "#FUNCTION F\n",
    "#THIS FUNCTION CREATES A COLUMN FOR THE SEARCH KEYWORD\n",
    "def keyword_series(final_keyword):\n",
    "    series = pd.Series(final_keyword)\n",
    "    final_series = series.repeat(40)\n",
    "    final_series = final_series.reset_index(drop=True)\n",
    "    return final_series\n",
    "    \n",
    "#FUNCTION G\n",
    "#THIS FUNCTION RETURNS A DATAFRAME FOR GIVEN WEBPAGE\n",
    "def firstpage_dataframe(final_keyword):\n",
    "    data = {'Keyword': keyword_series(final_keyword),\n",
    "            '原始url': article_links(),\n",
    "            '影响因子': impact_factor(),\n",
    "            '英文标题': article_titles(),\n",
    "            'pmid': get_pmid(),\n",
    "            '期刊': get_periodical()}\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.where(df['影响因子'] > 3.000)\n",
    "    #THIS FILTERING WORKS ONLY IF SEARCH KEYWORD IS IN ENGLISH\n",
    "    titles_column = df['英文标题']\n",
    "    titles = titles_column.str.contains(final_keyword, flags=re.IGNORECASE, regex=True)\n",
    "    df['Contains keyword'] = titles\n",
    "    df = df.where(df['Contains keyword'] == True)\n",
    "    df = df.dropna()\n",
    "    df = df.drop(columns=['Contains keyword'], axis = 1) \n",
    "    return df\n",
    "\n",
    "#CREATES THE FIRST DATAFRAME AND RECORDS NUMBER OF ROWS OF DATA EXTRACTED\n",
    "initial_df = firstpage_dataframe(final_keyword)\n",
    "length_df = len(initial_df)\n",
    "\n",
    "#FUNCTION H\n",
    "#THIS FUNCTION FINDS THE NEXT PAGE ON CHINESE PUB MED AND UPDATES current_page and search_url\n",
    "def findnextpage():\n",
    "    global current_page, search_url, result, soup\n",
    "    search_url = search_url + '&current_page=' + str(current_page + 1)\n",
    "    #Note: The sort by year is maintained as it carried over from the previous search url\n",
    "    current_page += 1\n",
    "    result = requests.get(search_url)\n",
    "    soup = BeautifulSoup(result.content, 'lxml')\n",
    "\n",
    "#FUNCTION I\n",
    "#THIS FUNCTION ADDS TO THE FIRST DATAFRAME AS MORE INFORMATION IS EXTRACTED\n",
    "def append_dataframe():\n",
    "    global initial_df, length_df\n",
    "    findnextpage()\n",
    "    updated_dataframe = initial_df.append(firstpage_dataframe(final_keyword))\n",
    "    length_df = len(updated_dataframe)\n",
    "    initial_df = updated_dataframe\n",
    "    return initial_df\n",
    "\n",
    "#CONTINUES TO EXTRACT INFORMATION MATCHING CRITERIA UNTIL FIRST 40 ARTICLES HAVE BEEN FOUND\n",
    "while length_df < 40:\n",
    "     append_dataframe()\n",
    "\n",
    "#RESETS INDEX OF FINAL DATAFRAME AND EXPORTS AS CSV FILE\n",
    "initial_df = initial_df.reset_index(drop=True)\n",
    "initial_df.to_csv(final_keyword+'_first_40.csv',index=False, encoding='utf_8_sig')\n",
    "print(\"Success!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
